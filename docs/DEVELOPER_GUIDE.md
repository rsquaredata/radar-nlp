# Guide D√©veloppeur - Job Radar

<div align="center">

**Version 1.0 | Documentation Technique**

*Architecture, APIs, et Contribution au Code*

[Accueil](../README.md) ‚Ä¢ [ Guide User](USER_GUIDE.md) ‚Ä¢ [ Docker](DOCKER_GUIDE.md) ‚Ä¢ [ Donn√©es](DATA_GUIDE.md)

---

</div>

##  Table des mati√®res

1. [Architecture](#architecture)
2. [Installation Dev](#installation-d√©veloppeur)
3. [Structure du Code](#structure-du-code)
4. [Base de Donn√©es](#base-de-donn√©es)
5. [Pipeline ETL](#pipeline-etl)
6. [APIs & Int√©grations](#apis--int√©grations)
7. [NLP & Machine Learning](#nlp--machine-learning)
8. [Frontend Streamlit](#frontend-streamlit)
9. [Tests](#tests)
10. [CI/CD](#cicd)
11. [Contribution](#contribution)
12. [Bonnes Pratiques](#bonnes-pratiques)

---

## üèõÔ∏è Architecture

### Vue d'ensemble

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      ARCHITECTURE GLOBALE                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Scraping  ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ     ETL     ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Database   ‚îÇ
‚îÇ  (Collecte) ‚îÇ    ‚îÇ (Transform) ‚îÇ    ‚îÇ   (SQLite)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                              ‚îÇ
                                              ‚ñº
                                       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                       ‚îÇ   Backend    ‚îÇ
                                       ‚îÇ (Python/SQL) ‚îÇ
                                       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                              ‚îÇ
                                              ‚ñº
                                       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                       ‚îÇ   Frontend   ‚îÇ
                                       ‚îÇ  (Streamlit) ‚îÇ
                                       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                              ‚îÇ
                                  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                  ‚ñº                       ‚ñº
                           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                           ‚îÇ NLP/ML   ‚îÇ          ‚îÇ    IA    ‚îÇ
                           ‚îÇ(scikit)  ‚îÇ          ‚îÇ(Mistral) ‚îÇ
                           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Stack Technique

| Layer | Technologies |
|-------|--------------|
| **Collecte** | BeautifulSoup, Requests, France Travail API |
| **ETL** | Pandas, SQLite, Custom Pipeline |
| **Backend** | Python 3.11+, SQLite3, Pandas |
| **Frontend** | Streamlit 1.32+, HTML/CSS/JS |
| **NLP** | scikit-learn, NLTK, spaCy |
| **ML** | K-Means, TF-IDF, Cosine Similarity |
| **IA** | Mistral AI API |
| **Viz** | Plotly, Folium, Matplotlib |
| **DevOps** | Docker, Docker Compose, GitHub Actions |

---

##  Installation D√©veloppeur

### Pr√©requis

```bash
# Versions requises
Python >= 3.11
pip >= 23.0
git >= 2.40
```

### Setup Complet

```bash
# 1. Cloner le repo
git clone https://github.com/rsquaredata/radar-nlp.git
cd job-radar

# 2. Cr√©er l'environnement virtuel
python -m venv venv

# Activer (Linux/Mac)
source venv/bin/activate

# Activer (Windows)
venv\Scripts\activate

# 3. Installer les d√©pendances
pip install -r requirements.txt
pip install -r requirements-dev.txt  # Outils de dev

# 4. Configurer les variables d'environnement
cp .env.example .env
nano .env

# 5. Initialiser la base de donn√©es
python database/etl_pipeline.py --input data/raw/jobs.csv --recreate

# 6. Lancer en mode dev
streamlit run app/home.py --server.runOnSave true
```

### Requirements Dev

```txt
# requirements-dev.txt
pytest>=7.4.0
pytest-cov>=4.1.0
black>=23.3.0
flake8>=6.0.0
mypy>=1.4.0
pre-commit>=3.3.0
ipython>=8.14.0
jupyter>=1.0.0
```

---

## üìÅ Structure du Code

### Arborescence D√©taill√©e

```python
projet_nlp/
‚îÇ
‚îú‚îÄ‚îÄ app/                          # Frontend Streamlit
‚îÇ   ‚îú‚îÄ‚îÄ home.py                   # Page d'accueil (entry point)
‚îÇ   ‚îú‚îÄ‚îÄ pages/                    # Pages de l'app
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Explorer.py           # Exploration offres
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Geographie.py         # Cartographie
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Analytics.py          # Statistiques
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Intelligence.py       # NLP/ML
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Assistant.py          # IA Mistral
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Contribuer.py         # Ajout offres
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ utils/                    # Utilitaires
‚îÇ       ‚îú‚îÄ‚îÄ components.py         # Composants UI r√©utilisables
‚îÇ       ‚îú‚îÄ‚îÄ db.py                 # Gestion BDD (CRUD)
‚îÇ       ‚îú‚îÄ‚îÄ nlp_utils.py          # Fonctions NLP
‚îÇ       ‚îú‚îÄ‚îÄ clustering.py         # K-Means & clustering
‚îÇ       ‚îú‚îÄ‚îÄ tfidf_analysis.py     # Analyse TF-IDF
‚îÇ       ‚îú‚îÄ‚îÄ viz.py                # Graphiques Plotly
‚îÇ       ‚îú‚îÄ‚îÄ filters.py            # Filtres de donn√©es
‚îÇ       ‚îî‚îÄ‚îÄ config.py             # Configuration globale
‚îÇ
‚îú‚îÄ‚îÄ database/                     # Base de donn√©es
‚îÇ   ‚îú‚îÄ‚îÄ schema.sql                # DDL (CREATE TABLE, etc.)
‚îÇ   ‚îú‚îÄ‚îÄ etl_pipeline.py           # Pipeline ETL complet
‚îÇ   ‚îú‚îÄ‚îÄ db_insert.py              # Fonctions d'insertion
‚îÇ   ‚îî‚îÄ‚îÄ jobs.db                   # Base SQLite
‚îÇ
‚îú‚îÄ‚îÄ scraping/                     # Collecte donn√©es
‚îÇ   ‚îú‚îÄ‚îÄ france_travail_api.py     # Client API France Travail
‚îÇ   ‚îú‚îÄ‚îÄ hellowork_scraper.py      # Scraper HelloWork
‚îÇ   ‚îî‚îÄ‚îÄ enrichment.py             # Enrichissement post-scraping
‚îÇ
‚îú‚îÄ‚îÄ geographic_enrichment/        # G√©olocalisation
‚îÇ   ‚îú‚îÄ‚îÄ enrich_geo.py             # Enrichissement g√©o
‚îÇ   ‚îî‚îÄ‚îÄ regions_france.json       # R√©f√©rentiel r√©gions
‚îÇ
‚îú‚îÄ‚îÄ data/                         # Donn√©es
‚îÇ   ‚îú‚îÄ‚îÄ raw/                      # Donn√©es brutes (CSV)
‚îÇ   ‚îú‚îÄ‚îÄ processed/                # Donn√©es trait√©es
‚îÇ   ‚îî‚îÄ‚îÄ exports/                  # Exports utilisateurs
‚îÇ
‚îÇ
‚îú‚îÄ‚îÄ docs/                         # Documentation
‚îÇ   ‚îú‚îÄ‚îÄ USER_GUIDE.md
‚îÇ   ‚îú‚îÄ‚îÄ DEVELOPER_GUIDE.md
‚îÇ   ‚îú‚îÄ‚îÄ DOCKER_GUIDE.md
‚îÇ   ‚îî‚îÄ‚îÄ DATA_GUIDE.md
‚îÇ

‚îú‚îÄ‚îÄ Dockerfile                    # Image Docker
‚îú‚îÄ‚îÄ docker-compose.yml            # Orchestration
‚îú‚îÄ‚îÄ requirements.txt              # D√©pendances prod        # D√©pendances dev                 # Variables d'env
‚îú‚îÄ‚îÄ .gitignore
‚îî‚îÄ‚îÄ README.md
```

### Conventions de nommage

```python
# Fichiers
snake_case.py          # Modules Python
PascalCase.py          # Pages Streamlit

# Classes
class DatabaseManager:  # PascalCase

# Fonctions
def load_offers():      # snake_case

# Variables
offer_count = 42        # snake_case
API_KEY = "xxx"         # UPPER_CASE pour constantes

# Constantes
DATABASE_PATH = Path("database/jobs.db")
MAX_RESULTS = 1000
```

---

## üóÑÔ∏è Base de Donn√©es

### Sch√©ma Relationnel

```sql
-- Star Schema optimis√© pour l'analyse

-- Dimension : Sources
CREATE TABLE dim_source (
    source_key INTEGER PRIMARY KEY AUTOINCREMENT,
    source_name TEXT NOT NULL UNIQUE,
    source_type TEXT  -- 'api', 'scraping'
);

-- Dimension : R√©gions
CREATE TABLE dim_region (
    region_key INTEGER PRIMARY KEY AUTOINCREMENT,
    region_name TEXT NOT NULL UNIQUE,
    latitude REAL,
    longitude REAL
);

-- Dimension : Entreprises
CREATE TABLE dim_company (
    company_key INTEGER PRIMARY KEY AUTOINCREMENT,
    company_name TEXT NOT NULL
);

-- Dimension : Contrats
CREATE TABLE dim_contract (
    contract_key INTEGER PRIMARY KEY AUTOINCREMENT,
    contract_type TEXT NOT NULL UNIQUE
);

-- Dimension : Comp√©tences
CREATE TABLE dim_skill (
    skill_key INTEGER PRIMARY KEY AUTOINCREMENT,
    skill_name TEXT NOT NULL UNIQUE,
    skill_type TEXT CHECK(skill_type IN ('competences', 'savoir_etre')),
    skill_category TEXT
);

-- Fait : Offres (CENTRALE)
CREATE TABLE fact_offers (
    offer_key INTEGER PRIMARY KEY AUTOINCREMENT,
    uid TEXT NOT NULL UNIQUE,  -- Hash MD5 pour d√©duplication
    offer_id TEXT NOT NULL,
    
    -- Foreign keys
    source_key INTEGER,
    region_key INTEGER,
    company_key INTEGER,
    contract_key INTEGER,
    
    -- Attributs
    title TEXT NOT NULL,
    source_url TEXT,           -- URL de l'offre
    location TEXT,
    salary TEXT,
    remote TEXT,
    published_date TEXT,
    description TEXT,
    
    -- M√©triques
    skills_count INTEGER DEFAULT 0,
    competences_count INTEGER DEFAULT 0,
    savoir_etre_count INTEGER DEFAULT 0,
    
    -- Tra√ßabilit√©
    added_by TEXT DEFAULT 'import',
    added_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    FOREIGN KEY (source_key) REFERENCES dim_source(source_key),
    FOREIGN KEY (region_key) REFERENCES dim_region(region_key),
    FOREIGN KEY (company_key) REFERENCES dim_company(company_key),
    FOREIGN KEY (contract_key) REFERENCES dim_contract(contract_key)
);

-- Table associative Many-to-Many
CREATE TABLE fact_offer_skill (
    offer_key INTEGER NOT NULL,
    skill_key INTEGER NOT NULL,
    PRIMARY KEY (offer_key, skill_key),
    FOREIGN KEY (offer_key) REFERENCES fact_offers(offer_key) ON DELETE CASCADE,
    FOREIGN KEY (skill_key) REFERENCES dim_skill(skill_key) ON DELETE CASCADE
);
```

### Index de Performance

```sql
-- Index unique sur UID (√©vite doublons)
CREATE UNIQUE INDEX idx_fact_offers_uid_unique 
ON fact_offers(uid);

-- Index sur cl√©s √©trang√®res
CREATE INDEX idx_fact_offers_source ON fact_offers(source_key);
CREATE INDEX idx_fact_offers_region ON fact_offers(region_key);
CREATE INDEX idx_fact_offers_company ON fact_offers(company_key);
CREATE INDEX idx_fact_offers_contract ON fact_offers(contract_key);

-- Index sur colonnes de recherche
CREATE INDEX idx_fact_offers_title ON fact_offers(title);
CREATE INDEX idx_fact_offers_location ON fact_offers(location);

-- Index sur table associative
CREATE INDEX idx_fact_offer_skill_offer ON fact_offer_skill(offer_key);
CREATE INDEX idx_fact_offer_skill_skill ON fact_offer_skill(skill_key);
```

### Vues Mat√©rialis√©es

```sql
-- Vue : Offres compl√®tes avec dimensions
CREATE VIEW v_offers_complete AS
SELECT 
    fo.offer_key,
    fo.uid,
    fo.title,
    fo.source_url,  -- Important pour le bouton Postuler
    fo.description,
    dr.region_name,
    dc.company_name,
    dct.contract_type,
    ds.source_name
FROM fact_offers fo
LEFT JOIN dim_region dr ON fo.region_key = dr.region_key
LEFT JOIN dim_company dc ON fo.company_key = dc.company_key
LEFT JOIN dim_contract dct ON fo.contract_key = dct.contract_key
LEFT JOIN dim_source ds ON fo.source_key = ds.source_key;

-- Vue : Top comp√©tences avec stats
CREATE VIEW v_top_skills AS
SELECT 
    ds.skill_name,
    ds.skill_type,
    COUNT(DISTINCT fos.offer_key) AS offer_count,
    ROUND(COUNT(DISTINCT fos.offer_key) * 100.0 / 
          (SELECT COUNT(*) FROM fact_offers), 2) AS percentage
FROM dim_skill ds
JOIN fact_offer_skill fos ON ds.skill_key = fos.skill_key
GROUP BY ds.skill_key
ORDER BY offer_count DESC;
```

### API Python (utils/db.py)

```python
import sqlite3
import pandas as pd
from pathlib import Path

class DatabaseManager:
    """Gestionnaire de connexion BDD"""
    
    def __init__(self, db_path: Path):
        self.db_path = db_path
        self.conn = None
    
    def get_connection(self):
        """Connexion SQLite"""
        return sqlite3.connect(self.db_path)
    
    def get_offers_with_skills(self) -> pd.DataFrame:
        """R√©cup√®re les offres avec comp√©tences agr√©g√©es"""
        query = """
            SELECT 
                fo.offer_key,
                fo.uid,
                fo.title,
                fo.source_url,  -- IMPORTANT !
                fo.description,
                dr.region_name,
                dc.company_name,
                dct.contract_type,
                GROUP_CONCAT(ds.skill_name) as all_skills
            FROM fact_offers fo
            LEFT JOIN fact_offer_skill fos ON fo.offer_key = fos.offer_key
            LEFT JOIN dim_skill ds ON fos.skill_key = ds.skill_key
            LEFT JOIN dim_region dr ON fo.region_key = dr.region_key
            LEFT JOIN dim_company dc ON fo.company_key = dc.company_key
            LEFT JOIN dim_contract dct ON fo.contract_key = dct.contract_key
            GROUP BY fo.offer_key
        """
        return pd.read_sql_query(query, self.get_connection())
    
    def search_offers(self, 
                     search_text: str = None,
                     regions: list = None,
                     contracts: list = None) -> pd.DataFrame:
        """Recherche avec filtres"""
        query = "SELECT * FROM v_offers_complete WHERE 1=1"
        params = []
        
        if search_text:
            query += " AND (title LIKE ? OR description LIKE ?)"
            pattern = f"%{search_text}%"
            params.extend([pattern, pattern])
        
        if regions:
            placeholders = ','.join(['?' for _ in regions])
            query += f" AND region_name IN ({placeholders})"
            params.extend(regions)
        
        if contracts:
            placeholders = ','.join(['?' for _ in contracts])
            query += f" AND contract_type IN ({placeholders})"
            params.extend(contracts)
        
        conn = self.get_connection()
        return pd.read_sql_query(query, conn, params=params)
```

---

## üîÑ Pipeline ETL

### Architecture ETL

```python
# database/etl_pipeline.py

class ETLPipeline:
    """Pipeline Extract-Transform-Load"""
    
    def __init__(self, db_path: str):
        self.db_path = db_path
        self.conn = None
        self.stats = {
            'offers_inserted': 0,
            'offers_duplicates': 0,
            'skills_inserted': 0
        }
    
    def extract(self, csv_path: str) -> pd.DataFrame:
        """1. EXTRACT : Charger les donn√©es brutes"""
        df = pd.read_csv(csv_path)
        print(f"‚úÖ {len(df)} lignes extraites")
        return df
    
    def transform(self, df: pd.DataFrame) -> pd.DataFrame:
        """2. TRANSFORM : Nettoyer et enrichir"""
        # Nettoyage
        df = df.dropna(subset=['title', 'company'])
        df['title'] = df['title'].str.strip()
        
        # G√©n√©ration UID unique (d√©duplication)
        df['uid'] = df.apply(
            lambda row: hashlib.md5(
                f"{row['offer_id']}{row['source']}".encode()
            ).hexdigest(),
            axis=1
        )
        
        # Extraction comp√©tences
        df['competences'] = df['description'].apply(self.extract_skills)
        
        print(f"‚úÖ {len(df)} lignes transform√©es")
        return df
    
    def load(self, df: pd.DataFrame):
        """3. LOAD : Insertion dans la BDD"""
        # Charger dimensions
        self.load_dimensions(df)
        
        # Charger faits
        self.load_offers(df)
        
        # Charger associations
        self.load_offer_skills(df)
        
        print(f"‚úÖ {self.stats['offers_inserted']} offres ins√©r√©es")
    
    def extract_skills(self, description: str) -> list:
        """Extraction de comp√©tences par regex + NLP"""
        skills = []
        
        # Patterns de comp√©tences techniques
        patterns = [
            r'\bPython\b', r'\bSQL\b', r'\bDocker\b',
            r'\bKubernetes\b', r'\bMachine Learning\b',
            r'\bTensorFlow\b', r'\bPyTorch\b'
        ]
        
        for pattern in patterns:
            if re.search(pattern, description, re.IGNORECASE):
                skills.append(pattern.replace(r'\b', ''))
        
        return skills

# Utilisation
if __name__ == "__main__":
    etl = ETLPipeline(db_path="database/jobs.db")
    
    # Extract
    df = etl.extract("data/raw/jobs.csv")
    
    # Transform
    df = etl.transform(df)
    
    # Load
    etl.load(df)
```

### D√©duplication

```python
def check_duplicate(self, uid: str) -> bool:
    """V√©rifie si une offre existe d√©j√†"""
    query = "SELECT COUNT(*) FROM fact_offers WHERE uid = ?"
    cursor = self.conn.execute(query, (uid,))
    count = cursor.fetchone()[0]
    return count > 0

def insert_offer(self, row: dict):
    """Insert avec gestion des doublons"""
    if self.check_duplicate(row['uid']):
        self.stats['offers_duplicates'] += 1
        return
    
    # Insertion
    query = """
        INSERT INTO fact_offers (uid, offer_id, title, ...)
        VALUES (?, ?, ?, ...)
    """
    self.conn.execute(query, (row['uid'], row['offer_id'], ...))
    self.stats['offers_inserted'] += 1
```

---

## üîå APIs & Int√©grations

### France Travail API

```python
# scraping/france_travail_api.py

import requests
from typing import List, Dict

class FranceTravailAPI:
    """Client API France Travail"""
    
    BASE_URL = "https://api.francetravail.io/partenaire/offresdemploi/v2"
    
    def __init__(self, client_id: str, client_secret: str):
        self.client_id = client_id
        self.client_secret = client_secret
        self.access_token = None
    
    def authenticate(self):
        """Authentification OAuth2"""
        url = "https://entreprise.francetravail.fr/connexion/oauth2/access_token"
        
        data = {
            'grant_type': 'client_credentials',
            'client_id': self.client_id,
            'client_secret': self.client_secret,
            'scope': 'api_offresdemploiv2 o2dsoffre'
        }
        
        response = requests.post(url, data=data)
        response.raise_for_status()
        
        self.access_token = response.json()['access_token']
    
    def search_offers(self, 
                     keywords: str = None,
                     location: str = None,
                     contract_type: str = None,
                     max_results: int = 150) -> List[Dict]:
        """Recherche d'offres"""
        
        if not self.access_token:
            self.authenticate()
        
        headers = {
            'Authorization': f'Bearer {self.access_token}',
            'Content-Type': 'application/json'
        }
        
        params = {
            'motsCles': keywords,
            'commune': location,
            'typeContrat': contract_type,
            'range': f'0-{max_results}'
        }
        
        response = requests.get(
            f"{self.BASE_URL}/search",
            headers=headers,
            params=params
        )
        response.raise_for_status()
        
        return response.json()['resultats']

# Utilisation
api = FranceTravailAPI(
    client_id=os.getenv('FRANCE_TRAVAIL_CLIENT_ID'),
    client_secret=os.getenv('FRANCE_TRAVAIL_CLIENT_SECRET')
)

offers = api.search_offers(
    keywords="Data Scientist",
    location="Paris",
    contract_type="CDI"
)
```

### HelloWork Scraper

```python
# scraping/hellowork_scraper.py

from bs4 import BeautifulSoup
import requests
from typing import List, Dict

class HelloWorkScraper:
    """Scraper HelloWork"""
    
    BASE_URL = "https://www.hellowork.com"
    
    def scrape_search_page(self, 
                          keywords: str,
                          location: str = None,
                          page: int = 1) -> List[Dict]:
        """Scrape une page de r√©sultats"""
        
        url = f"{self.BASE_URL}/fr-fr/emplois/recherche.html"
        params = {
            'k': keywords,
            'l': location,
            'p': page
        }
        
        response = requests.get(url, params=params)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        offers = []
        for card in soup.find_all('article', class_='job-card'):
            offer = {
                'title': card.find('h2').text.strip(),
                'company': card.find('p', class_='company').text.strip(),
                'location': card.find('span', class_='location').text.strip(),
                'url': self.BASE_URL + card.find('a')['href'],
                'contract_type': self.extract_contract_type(card)
            }
            offers.append(offer)
        
        return offers
    
    def scrape_offer_details(self, offer_url: str) -> Dict:
        """Scrape les d√©tails d'une offre"""
        
        response = requests.get(offer_url)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        return {
            'description': soup.find('div', class_='description').text.strip(),
            'salary': self.extract_salary(soup),
            'skills': self.extract_skills(soup)
        }

# Utilisation
scraper = HelloWorkScraper()
offers = scraper.scrape_search_page("Data Scientist", "Paris")
```

### Mistral AI Integration

```python
# app/utils/mistral_client.py

from mistralai.client import MistralClient
from mistralai.models.chat_completion import ChatMessage

class MistralAssistant:
    """Client Mistral AI pour l'assistant"""
    
    def __init__(self, api_key: str):
        self.client = MistralClient(api_key=api_key)
        self.model = "mistral-large-latest"
    
    def chat(self, 
             user_message: str,
             system_prompt: str = None,
             context: dict = None) -> str:
        """Discussion avec l'assistant"""
        
        messages = []
        
        # System prompt
        if system_prompt:
            messages.append(
                ChatMessage(role="system", content=system_prompt)
            )
        
        # Context (donn√©es du march√©)
        if context:
            context_str = self.format_context(context)
            messages.append(
                ChatMessage(role="system", content=context_str)
            )
        
        # User message
        messages.append(
            ChatMessage(role="user", content=user_message)
        )
        
        # API call
        response = self.client.chat(
            model=self.model,
            messages=messages
        )
        
        return response.choices[0].message.content
    
    def format_context(self, context: dict) -> str:
        """Formatte le contexte du march√©"""
        return f"""
        Contexte du march√© de l'emploi :
        - Total offres : {context['total_offers']}
        - Top 3 comp√©tences : {', '.join(context['top_skills'])}
        - R√©gions principales : {', '.join(context['top_regions'])}
        """

# Utilisation
assistant = MistralAssistant(api_key=os.getenv('MISTRAL_API_KEY'))

response = assistant.chat(
    user_message="Quelles sont les tendances du march√© ?",
    context={
        'total_offers': 2520,
        'top_skills': ['Python', 'SQL', 'Docker'],
        'top_regions': ['√éle-de-France', 'Auvergne', 'Occitanie']
    }
)
