# ğŸ“Š Guide des DonnÃ©es - Job Radar

<div align="center">

**ModÃ©lisation, ETL, et Analyse des DonnÃ©es**

*Comprendre la structure et le traitement des donnÃ©es*

[ Accueil](../README.md) â€¢ [ User](USER_GUIDE.md) â€¢ [ğŸ”§ Dev](DEVELOPER_GUIDE.md) â€¢ [ Docker](DOCKER_GUIDE.md)

---

</div>

## ğŸ“‹ Table des matiÃ¨res

1. [Vue d'ensemble](#-vue-densemble)
2. [Sources de donnÃ©es](#-sources-de-donnÃ©es)
3. [ModÃ¨le de donnÃ©es](#-modÃ¨le-de-donnÃ©es)
4. [Pipeline ETL](#-pipeline-etl)
5. [QualitÃ© des donnÃ©es](#-qualitÃ©-des-donnÃ©es)
6. [RequÃªtes SQL](#-requÃªtes-sql)
7. [Export & APIs](#-export--apis)
8. [Maintenance](#-maintenance)

---

##  Vue d'ensemble

### Chiffres clÃ©s

```
 2,520 offres d'emploi
 890+ entreprises
 53 rÃ©gions franÃ§aises
 500+ compÃ©tences uniques
 Mise Ã  jour quotidienne
 DonnÃ©es depuis novembre 2024
```

### Architecture des donnÃ©es

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  ARCHITECTURE DONNÃ‰ES                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  Sources                ETL              Stockage
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ France  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚Extract â”‚â”€â”€â”€â”€â”€â”€â”€â–¶â”‚          â”‚
â”‚Travail  â”‚          â”‚        â”‚        â”‚ SQLite   â”‚
â”‚  API    â”‚          â”‚Transformâ”‚        â”‚  Star    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚        â”‚        â”‚ Schema   â”‚
                     â”‚ Load   â”‚        â”‚          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚        â”‚        â”‚  2,520   â”‚
â”‚HelloWorkâ”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚NLP/Geo â”‚â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  Rows    â”‚
â”‚Scraping â”‚          â”‚Enrich  â”‚        â”‚          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”Œ Sources de donnÃ©es

### 1. France Travail API

**Type :** API REST officielle  
**URL :** `https://api.francetravail.io/partenaire/offresdemploi/v2`  
**Volume :** ~1,500 offres  
**FrÃ©quence :** Quotidienne

**Exemple de donnÃ©es :**

```json
{
  "id": "174XBFZ",
  "intitule": "Data Scientist H/F",
  "description": "Nous recherchons un Data Scientist...",
  "dateCreation": "2025-01-10T09:30:00Z",
  "lieuTravail": {
    "libelle": "75 - PARIS 15",
    "latitude": 48.8534,
    "longitude": 2.3488
  },
  "entreprise": {
    "nom": "GOOGLE FRANCE",
    "description": "Leader mondial..."
  },
  "typeContrat": "CDI",
  "salaire": {
    "libelle": "Annuel de 50000.00 Euros Ã  75000.00 Euros"
  },
  "competences": [
    {
      "libelle": "Python",
      "exigence": "E"
    },
    {
      "libelle": "Machine Learning",
      "exigence": "S"
    }
  ]
}
```

**Mapping vers notre modÃ¨le :**

| API Field | Notre Field | Transformation |
|-----------|-------------|----------------|
| `id` | `offer_id` | Direct |
| `intitule` | `title` | Nettoyage |
| `description` | `description` | NLP extraction |
| `entreprise.nom` | `company_name` | Normalisation |
| `typeContrat` | `contract_type` | Mapping CDI/CDD/... |
| `lieuTravail.libelle` | `location` | GÃ©o-enrichissement |

### 2. HelloWork (Web Scraping)

**Type :** Scraping HTML  
**URL :** `https://www.hellowork.com/fr-fr/emplois/`  
**Volume :** ~1,000 offres  
**FrÃ©quence :** Hebdomadaire

**Structure HTML :**

```html
<article class="job-card">
  <h2 class="job-title">Data Scientist Senior</h2>
  <p class="company-name">Google France</p>
  <span class="location">Paris (75)</span>
  <span class="contract">CDI</span>
  <a href="/emplois/123456.html" class="job-link">
    Voir l'offre
  </a>
  <div class="job-description">
    Nous recherchons un Data Scientist...
  </div>
</article>
```

**Code d'extraction :**

```python
from bs4 import BeautifulSoup

def scrape_hellowork(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')
    
    offers = []
    for card in soup.find_all('article', class_='job-card'):
        offer = {
            'title': card.find('h2').text.strip(),
            'company': card.find('p', class_='company-name').text.strip(),
            'location': card.find('span', class_='location').text.strip(),
            'contract_type': card.find('span', class_='contract').text.strip(),
            'url': 'https://www.hellowork.com' + card.find('a')['href'],
            'description': card.find('div', class_='job-description').text.strip()
        }
        offers.append(offer)
    
    return offers
```

---

##  ModÃ¨le de donnÃ©es

### Star Schema

```sql
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  dim_region  â”‚
                    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
                    â”‚ region_key PKâ”‚
                    â”‚ region_name  â”‚
                    â”‚ latitude     â”‚
                    â”‚ longitude    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚dim_sourceâ”‚          â”‚        â”‚ dim_company â”‚
    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚          â”‚        â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
    â”‚source_keyâ”‚          â”‚        â”‚ company_key â”‚
    â”‚name      â”‚          â”‚        â”‚ company_nameâ”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜          â”‚        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
         â”‚                â”‚               â”‚
         â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
         â””â”€â”€â”€â”€â”‚    fact_offers        â”‚â”€â”€â”€â”˜
              â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
              â”‚ offer_key PK          â”‚
              â”‚ uid UNIQUE            â”‚â—€â”€â”€ DÃ©duplication
              â”‚ title                 â”‚
              â”‚ description           â”‚
              â”‚ source_url            â”‚â—€â”€â”€ Important !
              â”‚ salary                â”‚
              â”‚ remote                â”‚
              â”‚ source_key FK         â”‚
              â”‚ region_key FK         â”‚
              â”‚ company_key FK        â”‚
              â”‚ contract_key FK       â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â”‚ N:M
                          â”‚
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚fact_offer_skill   â”‚
                  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
                  â”‚ offer_key FK      â”‚
                  â”‚ skill_key FK      â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚   dim_skill   â”‚
                  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
                  â”‚ skill_key PK  â”‚
                  â”‚ skill_name    â”‚
                  â”‚ skill_type    â”‚
                  â”‚ skill_categoryâ”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Tables de dimensions

#### 1. dim_region (53 lignes)

```sql
CREATE TABLE dim_region (
    region_key INTEGER PRIMARY KEY AUTOINCREMENT,
    region_name TEXT NOT NULL UNIQUE,
    latitude REAL,
    longitude REAL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Exemple
INSERT INTO dim_region (region_name, latitude, longitude)
VALUES ('Ãle-de-France', 48.8566, 2.3522);
```

**DonnÃ©es :**

| region_key | region_name | latitude | longitude |
|------------|-------------|----------|-----------|
| 1 | Ãle-de-France | 48.8566 | 2.3522 |
| 2 | Auvergne-RhÃ´ne-Alpes | 45.7640 | 4.8357 |
| 3 | Occitanie | 43.6047 | 1.4442 |
| ... | ... | ... | ... |

#### 2. dim_skill (500+ lignes)

```sql
CREATE TABLE dim_skill (
    skill_key INTEGER PRIMARY KEY AUTOINCREMENT,
    skill_name TEXT NOT NULL UNIQUE,
    skill_type TEXT CHECK(skill_type IN ('competences', 'savoir_etre')),
    skill_category TEXT,  -- 'languages', 'cloud', 'tools'...
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Exemples
INSERT INTO dim_skill (skill_name, skill_type, skill_category)
VALUES 
    ('Python', 'competences', 'languages'),
    ('Machine Learning', 'competences', 'ai'),
    ('Communication', 'savoir_etre', 'soft_skills');
```

**Top 20 compÃ©tences :**

| Rang | skill_name | Occurrences | % |
|------|------------|-------------|---|
| 1 | Python | 1,689 | 67% |
| 2 | SQL | 1,210 | 48% |
| 3 | Machine Learning | 1,134 | 45% |
| 4 | Docker | 1,058 | 42% |
| 5 | TensorFlow | 957 | 38% |
| 6 | Kubernetes | 882 | 35% |
| 7 | Cloud (AWS/GCP/Azure) | 831 | 33% |
| 8 | Spark | 756 | 30% |
| 9 | Airflow | 693 | 28% |
| 10 | Git | 630 | 25% |

#### 3. dim_company (890+ lignes)

```sql
CREATE TABLE dim_company (
    company_key INTEGER PRIMARY KEY AUTOINCREMENT,
    company_name TEXT NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

**Top entreprises :**

| Entreprise | Offres |
|------------|--------|
| Google | 45 |
| Meta | 38 |
| Amazon | 35 |
| Microsoft | 32 |
| Apple | 28 |

### Table de faits

#### fact_offers (2,520 lignes)

```sql
CREATE TABLE fact_offers (
    offer_key INTEGER PRIMARY KEY AUTOINCREMENT,
    
    -- Identifiants
    uid TEXT NOT NULL UNIQUE,  -- MD5 hash pour dÃ©duplication
    offer_id TEXT NOT NULL,
    
    -- ClÃ©s Ã©trangÃ¨res
    source_key INTEGER,
    region_key INTEGER,
    company_key INTEGER,
    contract_key INTEGER,
    
    -- Attributs
    title TEXT NOT NULL,
    source_url TEXT,           
    location TEXT,
    salary TEXT,
    remote TEXT,
    published_date TEXT,
    description TEXT,
    
    -- MÃ©triques
    skills_count INTEGER DEFAULT 0,
    competences_count INTEGER DEFAULT 0,
    savoir_etre_count INTEGER DEFAULT 0,
    
    -- TraÃ§abilitÃ©
    added_by TEXT DEFAULT 'import',
    added_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    FOREIGN KEY (source_key) REFERENCES dim_source(source_key),
    FOREIGN KEY (region_key) REFERENCES dim_region(region_key),
    FOREIGN KEY (company_key) REFERENCES dim_company(company_key),
    FOREIGN KEY (contract_key) REFERENCES dim_contract(contract_key)
);
```

**Exemple de ligne :**

```
offer_key: 1
uid: 84b8ca8df812d5a779f3fbafe55ce8a0
title: Data Scientist Senior IA
source_url: https://www.hellowork.com/fr-fr/emplois/72337185.html
company_name: Talan
region_name: Ãle-de-France
contract_type: CDI
remote: yes
skills_count: 12
```

### Index de performance

```sql
-- Index unique sur UID (Ã©vite doublons)
CREATE UNIQUE INDEX idx_offers_uid ON fact_offers(uid);

-- Index sur clÃ©s Ã©trangÃ¨res
CREATE INDEX idx_offers_source ON fact_offers(source_key);
CREATE INDEX idx_offers_region ON fact_offers(region_key);
CREATE INDEX idx_offers_company ON fact_offers(company_key);

-- Index sur colonnes de recherche
CREATE INDEX idx_offers_title ON fact_offers(title);
CREATE INDEX idx_offers_remote ON fact_offers(remote);
```

---

##  Pipeline ETL

### Processus complet

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              PIPELINE ETL DÃ‰TAILLÃ‰              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. EXTRACT (Extraction)
   â”œâ”€ France Travail API
   â”‚  â””â”€ GET /offresdemploi/v2/search
   â”‚     â†’ JSON response
   â”‚     â†’ ~1,500 offres
   â”‚
   â””â”€ HelloWork Scraping
      â””â”€ Parse HTML avec BeautifulSoup
         â†’ Liste d'offres
         â†’ ~1,000 offres

2. TRANSFORM (Transformation)
   â”œâ”€ Nettoyage
   â”‚  â”œâ”€ Suppression doublons
   â”‚  â”œâ”€ Normalisation texte
   â”‚  â””â”€ Validation format
   â”‚
   â”œâ”€ Enrichissement
   â”‚  â”œâ”€ Extraction compÃ©tences (NLP)
   â”‚  â”‚  â””â”€ Regex + patterns + NER
   â”‚  â”œâ”€ GÃ©olocalisation
   â”‚  â”‚  â””â”€ Commune â†’ RÃ©gion
   â”‚  â””â”€ GÃ©nÃ©ration UID
   â”‚     â””â”€ MD5(offer_id + source)
   â”‚
   â””â”€ Structuration
      â””â”€ CSV â†’ DataFrame Pandas

3. LOAD (Chargement)
   â”œâ”€ Dimensions (INSERT OR IGNORE)
   â”‚  â”œâ”€ dim_region
   â”‚  â”œâ”€ dim_company
   â”‚  â”œâ”€ dim_contract
   â”‚  â””â”€ dim_skill
   â”‚
   â”œâ”€ Faits (INSERT OR IGNORE)
   â”‚  â””â”€ fact_offers (avec uid unique)
   â”‚
   â””â”€ Associations (INSERT OR IGNORE)
      â””â”€ fact_offer_skill (N:M)
```

### Code ETL (simplifiÃ©)

```python
# database/etl_pipeline.py

class ETLPipeline:
    def run(self, csv_path: str):
        # 1. EXTRACT
        df = pd.read_csv(csv_path)
        print(f"âœ… {len(df)} offres extraites")
        
        # 2. TRANSFORM
        df = self.clean_data(df)
        df = self.enrich_data(df)
        df = self.deduplicate(df)
        print(f"âœ… {len(df)} offres transformÃ©es")
        
        # 3. LOAD
        self.load_dimensions(df)
        self.load_facts(df)
        self.load_associations(df)
        print(f"âœ… {len(df)} offres chargÃ©es")
    
    def clean_data(self, df):
        """Nettoyage"""
        df = df.dropna(subset=['title', 'company'])
        df['title'] = df['title'].str.strip()
        df['description'] = df['description'].str.lower()
        return df
    
    def enrich_data(self, df):
        """Enrichissement"""
        # Extraction compÃ©tences
        df['competences'] = df['description'].apply(
            self.extract_skills
        )
        
        # GÃ©olocalisation
        df['region'] = df['location'].apply(
            self.geocode_region
        )
        
        # GÃ©nÃ©ration UID
        df['uid'] = df.apply(
            lambda row: hashlib.md5(
                f"{row['offer_id']}{row['source']}".encode()
            ).hexdigest(),
            axis=1
        )
        
        return df
    
    def extract_skills(self, description):
        """Extraction NLP"""
        skills = []
        
        # Patterns regex
        patterns = {
            'Python': r'\bPython\b',
            'SQL': r'\bSQL\b',
            'Docker': r'\bDocker\b',
            'Machine Learning': r'\bMachine Learning\b|\bML\b'
        }
        
        for skill, pattern in patterns.items():
            if re.search(pattern, description, re.I):
                skills.append(skill)
        
        return skills
```

---

##  QualitÃ© des donnÃ©es

### MÃ©triques de qualitÃ©

```python
# Analyse de qualitÃ©
def data_quality_report(df):
    report = {
        'total_rows': len(df),
        'duplicates': df.duplicated(subset=['uid']).sum(),
        'missing_title': df['title'].isna().sum(),
        'missing_company': df['company'].isna().sum(),
        'missing_url': df['source_url'].isna().sum(),
        'invalid_dates': (df['published_date'] == 'N/A').sum(),
    }
    
    return report

# Exemple de rÃ©sultat
{
    'total_rows': 2520,
    'duplicates': 0,           # âœ… Aucun doublon
    'missing_title': 0,        # âœ… 100% complÃ©tude
    'missing_company': 23,     # âš ï¸ 0.9% manquant
    'missing_url': 0,          # âœ… Toutes les URLs prÃ©sentes
    'invalid_dates': 156       # âš ï¸ 6.2% dates invalides
}
```

### Validation des donnÃ©es

```sql
-- VÃ©rifier les doublons
SELECT uid, COUNT(*) as count
FROM fact_offers
GROUP BY uid
HAVING count > 1;

-- VÃ©rifier les URLs
SELECT COUNT(*) as offers_with_url
FROM fact_offers
WHERE source_url IS NOT NULL 
  AND source_url LIKE 'http%';

-- VÃ©rifier la complÃ©tude
SELECT 
    COUNT(*) as total,
    SUM(CASE WHEN title IS NULL THEN 1 ELSE 0 END) as missing_title,
    SUM(CASE WHEN source_url IS NULL THEN 1 ELSE 0 END) as missing_url
FROM fact_offers;
```

---

## ğŸ” RequÃªtes SQL

### RequÃªtes utiles

#### 1. Top 10 compÃ©tences

```sql
SELECT 
    ds.skill_name,
    COUNT(DISTINCT fos.offer_key) as offer_count,
    ROUND(COUNT(DISTINCT fos.offer_key) * 100.0 / 
          (SELECT COUNT(*) FROM fact_offers), 2) as percentage
FROM dim_skill ds
JOIN fact_offer_skill fos ON ds.skill_key = fos.skill_key
GROUP BY ds.skill_key
ORDER BY offer_count DESC
LIMIT 10;
```

#### 2. Offres par rÃ©gion

```sql
SELECT 
    dr.region_name,
    COUNT(fo.offer_key) as offer_count,
    AVG(fo.skills_count) as avg_skills
FROM dim_region dr
LEFT JOIN fact_offers fo ON dr.region_key = fo.region_key
GROUP BY dr.region_key
ORDER BY offer_count DESC;
```

#### 3. CompÃ©tences d'une offre

```sql
SELECT 
    fo.title,
    GROUP_CONCAT(ds.skill_name, ', ') as skills
FROM fact_offers fo
JOIN fact_offer_skill fos ON fo.offer_key = fos.offer_key
JOIN dim_skill ds ON fos.skill_key = ds.skill_key
WHERE fo.offer_key = 1
GROUP BY fo.offer_key;
```

#### 4. Offres avec tÃ©lÃ©travail par rÃ©gion

```sql
SELECT 
    dr.region_name,
    COUNT(CASE WHEN fo.remote IN ('yes', 'oui') THEN 1 END) as remote_count,
    COUNT(*) as total_count,
    ROUND(COUNT(CASE WHEN fo.remote IN ('yes', 'oui') THEN 1 END) * 100.0 / COUNT(*), 1) as remote_pct
FROM dim_region dr
JOIN fact_offers fo ON dr.region_key = fo.region_key
GROUP BY dr.region_key
ORDER BY remote_pct DESC;
```

---

## ğŸ“¤ Export & APIs

### Export CSV

```python
# Export complet
df = load_offers_with_skills()
df.to_csv('exports/all_offers.csv', index=False, encoding='utf-8')

# Export filtrÃ©
df_filtered = df[df['region_name'] == 'Ãle-de-France']
df_filtered.to_csv('exports/idf_offers.csv', index=False)
```

### Export JSON

```python
# Format JSON pour APIs
offers = df.to_dict('records')

with open('exports/offers.json', 'w', encoding='utf-8') as f:
    json.dump(offers, f, ensure_ascii=False, indent=2)
```

### API REST (exemple Flask)

```python
from flask import Flask, jsonify

app = Flask(__name__)

@app.route('/api/offers')
def get_offers():
    df = load_offers_with_skills()
    return jsonify(df.to_dict('records'))

@app.route('/api/offers/<int:offer_id>')
def get_offer(offer_id):
    offer = get_offer_by_id(offer_id)
    return jsonify(offer)

@app.route('/api/stats')
def get_stats():
    return jsonify({
        'total_offers': 2520,
        'total_companies': 890,
        'total_regions': 53
    })
```

---

## ğŸ”§ Maintenance

### Backup rÃ©gulier

```bash
# Backup quotidien
#!/bin/bash
DATE=$(date +%Y%m%d)
sqlite3 database/jobs.db ".backup database/backups/jobs_$DATE.db"

# Compression
gzip database/backups/jobs_$DATE.db

# Nettoyage (garde 30 jours)
find database/backups/ -mtime +30 -delete
```

### Mise Ã  jour des donnÃ©es

```bash
# Script de mise Ã  jour
python scraping/france_travail_api.py > data/raw/new_offers.csv
python database/etl_pipeline.py --input data/raw/new_offers.csv
```

### Optimisation

```sql
-- Reconstruire les index
REINDEX;

-- Analyser les tables
ANALYZE;

-- Vacuum (compacter la BDD)
VACUUM;
```

---

<div align="center">

** DonnÃ©es structurÃ©es = Analyses puissantes !**

[â¬†ï¸ Retour en haut](#-guide-des-donnÃ©es---job-radar)

</div>