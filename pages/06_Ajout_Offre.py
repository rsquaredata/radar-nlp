from __future__ import annotations

"""
Page 06_Ajout_Offre ‚Äì Ajout interactif d'une offre dans RADAR √† partir d'une URL.

Fonctionnement pr√©vu :
- l'utilisateur colle une URL d'offre (Indeed, Apec, Jooble, ...),
- la source est d√©tect√©e automatiquement (avec possibilit√© de forcer),
- on appelle le scraper / client API correspondant,
- on passe le r√©sultat dans process_offre() pour g√©n√©rer les variables structur√©es,
- on appelle upsert_offre_complete() pour ins√©rer / mettre √† jour l'offre dans DuckDB,
- on affiche un r√©capitulatif et un message de succ√®s.
"""

import re
from typing import Dict, Optional, Tuple
from urllib.parse import urlparse

import duckdb
import streamlit as st

from radar.db.schema import get_connection

# üß© √Ä adapter selon ton projet r√©el :
# (si les chemins ne sont pas bons, tu modifies juste ces imports)
try:
    from radar.scraping.indeed import fetch_indeed_offer
except ImportError:  # fallback symbolique pour √©viter que la page crashe
    fetch_indeed_offer = None  # type: ignore

try:
    from radar.scraping.apec import fetch_apec_offer
except ImportError:
    fetch_apec_offer = None  # type: ignore

try:
    from radar.scraping.jooble import fetch_jooble_offer
except ImportError:
    fetch_jooble_offer = None  # type: ignore

try:
    from radar.pipeline.process_offre import process_offre
except ImportError:
    process_offre = None  # type: ignore

try:
    from radar.db.io import upsert_offre_complete
except ImportError:
    upsert_offre_complete = None  # type: ignore


# =========================
# Connexion DB
# =========================

@st.cache_resource(show_spinner=False)
def get_cached_connection() -> duckdb.DuckDBPyConnection:
    """Connexion DuckDB mise en cache c√¥t√© Streamlit."""
    return get_connection()


# =========================
# D√©tection de la source
# =========================

def detect_source_from_url(url: str) -> Optional[str]:
    """
    D√©tecte la source √† partir du host de l'URL.
    Retourne l'un de : 'Indeed', 'Apec', 'Jooble', ou None si inconnu.
    """
    try:
        parsed = urlparse(url)
        host = parsed.netloc.lower()
    except Exception:
        return None

    if "indeed" in host:
        return "Indeed"
    if "apec" in host:
        return "Apec"
    if "jooble" in host:
        return "Jooble"
    return None


def get_scraper_for_source(source: str):
    """
    Retourne la fonction de scraping adapt√©e √† la source.
    L√®ve une ValueError si la source n'est pas support√©e ou si le scraper manque.
    """
    if source == "Indeed":
        if fetch_indeed_offer is None:
            raise ValueError("Scraper Indeed non disponible (fetch_indeed_offer introuvable).")
        return fetch_indeed_offer

    if source == "Apec":
        if fetch_apec_offer is None:
            raise ValueError("Scraper Apec non disponible (fetch_apec_offer introuvable).")
        return fetch_apec_offer

    if source == "Jooble":
        if fetch_jooble_offer is None:
            raise ValueError("Client Jooble non disponible (fetch_jooble_offer introuvable).")
        return fetch_jooble_offer

    raise ValueError(f"Source non support√©e : {source}")


# =========================
# Pipeline d'ajout d'offre
# =========================

def add_offer_from_url(
    url: str,
    source: str,
    conn: duckdb.DuckDBPyConnection,
) -> Tuple[Dict, Dict]:
    """
    Ex√©cute le pipeline complet :
    - scraping / API pour r√©cup√©rer raw_job,
    - process_offre(raw_job) -> offre_nlp,
    - upsert_offre_complete(offre_nlp) dans DuckDB.

    Retourne (raw_job, offre_nlp) pour affichage √©ventuel.
    """
    if process_offre is None:
        raise RuntimeError("process_offre() est introuvable. V√©rifie l'import radar.pipeline.process_offre.")

    if upsert_offre_complete is None:
        raise RuntimeError("upsert_offre_complete() est introuvable. V√©rifie l'import radar.db.io.")

    scraper = get_scraper_for_source(source)

    # 1. Scraping / API
    raw_job = scraper(url)
    if raw_job is None:
        raise RuntimeError("Le scraper n'a rien retourn√© (raw_job est vide ou None).")

    # 2. Traitement NLP / enrichissement
    offre_nlp = process_offre(raw_job)
    if offre_nlp is None:
        raise RuntimeError("process_offre() n'a rien retourn√© (offre_nlp est vide ou None).")

    # 3. Insertion en base
    # Selon ton impl√©mentation r√©elle : avec ou sans connexion en param√®tre
    try:
        upsert_offre_complete(conn, offre_nlp)  # type: ignore[arg-type]
    except TypeError:
        # fallback si ta fonction ne prend pas de connexion
        upsert_offre_complete(offre_nlp)  # type: ignore[call-arg]

    return raw_job, offre_nlp


# =========================
# UI principale
# =========================

def main() -> None:
    st.title("‚ûï Ajouter une nouvelle offre")

    st.markdown(
        """
Cette page permet d'ajouter **manuellement** une offre au corpus RADAR
√† partir d'une simple URL (Indeed, Apec, Jooble, ...).

Pipeline :
1. d√©tection / s√©lection de la source ;
2. scraping ou appel API pour r√©cup√©rer l'offre brute ;
3. passage dans `process_offre()` pour extraire les variables structur√©es ;
4. insertion / mise √† jour dans la base via `upsert_offre_complete()`.
"""
    )

    conn = get_cached_connection()

    st.markdown("### 1. Saisie de l'URL")

    url = st.text_input("URL de l'offre", placeholder="https://...")

    auto_source = detect_source_from_url(url) if url else None

    st.markdown("### 2. Source de l'offre")

    source_options = ["Indeed", "Apec", "Jooble"]
    default_index = 0

    if auto_source in source_options:
        default_index = source_options.index(auto_source)

    source = st.selectbox(
        "Source (d√©tect√©e automatiquement si possible)",
        options=source_options,
        index=default_index,
        help=(
            "La source est sugg√©r√©e √† partir du domaine de l'URL, "
            "mais vous pouvez la modifier manuellement."
        ),
    )

    st.markdown("### 3. Scraper et ajouter dans l'entrep√¥t")

    if st.button("Scraper & ajouter l'offre", type="primary", use_container_width=True):
        if not url:
            st.error("Merci de renseigner une URL d'offre.")
            return

        with st.spinner(f"Scraping de l'offre depuis {source}..."):
            try:
                raw_job, offre_nlp = add_offer_from_url(url, source, conn)
            except Exception as e:
                st.error(
                    "Une erreur est survenue lors de l'ajout de l'offre. "
                    "V√©rifiez les logs et l'impl√©mentation des scrapers / process_offre()."
                )
                st.exception(e)
                return

        st.success("Offre ajout√©e (ou mise √† jour) avec succ√®s dans l'entrep√¥t RADAR.")

        with st.expander("Voir les donn√©es brutes (raw_job)"):
            st.json(raw_job)

        with st.expander("Voir l'offre enrichie (offre_nlp)"):
            st.json(offre_nlp)

        st.info(
            "L'offre est d√©sormais disponible dans fact_offre (et tables associ√©es), "
            "et sera visible dans les autres pages (vue globale, carte, comp√©tences, NLP, comparaison)."
        )


if __name__ == "__main__":
    main()
